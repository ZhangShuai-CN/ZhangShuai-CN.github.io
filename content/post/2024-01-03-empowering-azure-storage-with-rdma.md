---
layout:     post
title:      "使用 RDMA 提升微软 Azure 云的存储性能 [NSDI'23]"
subtitle:   "使用 RDMA 提升微软 Azure 云的存储性能"
description: "Empowering Azure Storage with RDMA"
excerpt: ""
date:       2024-01-03 01:01:01
author:     "张帅"
image: "/img/2024-01-03-empowering-azure-storage-with-rdma/background.jpg"
showtoc: true
draft: true
tags:
    - RDMA
    - Storage
categories: [ Tech ]
URL: "/2024/01/03/empowering-azure-storage-with-rdma/"
---

- - -
###### 关于作者
> 
> **`张帅，网络从业人员，公众号：Flowlet`**
> 
> **`个人博客：https://flowlet.net/`**
- - -

## 序言
- - -

NSDI 的全称是 Networked Systems Design and Implementation，是 USENIX 旗下的旗舰会议之一，也是计算机网络系统领域久负盛名的顶级会议。与网络领域的另一顶会 SIGCOMM 相比，NSDI 更加侧重于网络系统的设计与实现。

RDMA 在数据中心的主要应用场景是存储与 HPC/AI，微软目前在所有服务器上都部署了 RDMA 网卡，在微软 Azure 云中 RDMA 流量已经占到了数据中心总流量的 70%，超过了传统的以太网流量。

本文翻译自 NSDI'23 论文《[Empowering Azure Storage with RDMA](https://www.usenix.org/conference/nsdi23/presentation/bai)》，该文章阐述了微软在 Azure 云中通过部署 RDMA 来提升存储性能。

## 前言
- - -

由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅[原文](https://www.usenix.org/conference/nsdi23/presentation/bai)。

## 摘要
- - -

鉴于公共云中广泛采用存算分离架构（Disaggregated Storage），网络是云存储服务实现高性能和高可靠性的关键。在 Azure 云中，我们在存储前端流量（计算 VM 和存储集群之间）和后端流量（存储集群内）之间启用 RDMA（Remote Direct Memory Access）作为我们的传输层。由于计算集群和存储集群可能位于 Azure 云 region 内的不同 dc 中，因此我们需要在 region 范围内支持 RDMA。

这项工作展示了我们通过在 region 内部署 RDMA 以承载 Azure 中的存储工作负载方面的经验。Azure 中的基础设施的高度复杂性和异构性同时带来了一系列新的挑战，例如不同类型的 RDMA 网卡之间的互操作性问题。我们对网络基础设施进行了多项更改以应对这些挑战。如今，在 Azure 中大约 70% 的流量是 RDMA 流量，并且在 Azure 的所有公有云 region 都支持 region 内 RDMA。RDMA 帮助我们显着的提升磁盘 I/O 性能并节​​省 CPU 资源。

## 1. 介绍
- - -

高性能、高可靠的存储服务是公有云最基础的服务之一。近年来，我们见证了存储介质和技术的显着改进，客户也希望能在云中能获得类似的性能提升。鉴于云中广泛采用存算分离架构，互连计算集群和存储集群的网络成为云存储的关键性能瓶颈。尽管基于 Clos 的网络架构提供了足够的网络带宽，但传统的 TCP/IP 协议栈仍面临高延迟、单核吞吐量低，CPU 消耗高等问题，使其并不适合这种存算分离的场景。

鉴于这些限制，RDMA（Remote Direct Memory Access） 提供了一种很有前景的解决方案。通过将网络协议栈卸载到网卡（NIC）硬件上，RDMA 在 CPU 接近零开销的情况下，实现了超低处理延迟和超高吞吐。除了性能改进之外，RDMA 还减少了每台服务器上专为网络协议栈处理报文所预留的 CPU 核数。这些节省下来的 CPU 核可以作为 VM 进行出售或被用于应用程序。

为了充分利用 RDMA 的优势，我们的目标是在存储前端流量（VM 计算集群和存储集群之间）和后端流量（存储集群内）中启用 RDMA。这与之前的工作不同，之前的工作仅仅是针对存储后端流量启用 RDMA。在 Azure 云中，由于受容量问题的影响，计算集群和存储集群可能位于同一 region 内的不同数据中心。这就要求我们必须具有 region 规模支持 RDMA 的能力。

在本文中，我们总结了在 region 规模部署 RDMA 以支持 Azure 存储工作负载的经验。与之前的 RDMA 部署相比，由于 Azure region 内的高度复杂性和异构性，region 规模 RDMA 部署引入了许多新的挑战。随着 Azure 基础设施的不断发展，不同的集群可能会部署不同的 RDMA NIC。虽然所有 NIC 都支持 DCQCN，但不同厂商 NIC 的实现方式不同。当不同厂商的 NIC 互通时，这会导致许多不可预期的行为。同样的，来自多个供应商的异构交换机软硬件显着增加了我们的运营工作量。此外，互连数据中心的长距离电缆会导致 region 内较大的传播时延和较大的往返时间（RTT）的变化，这给拥塞控制带来了新的挑战。

为了安全地在 region 内为 Azure 存储流量启用 RDMA，我们从应用层协议到链路层流量控制等方面对网络基础设施进行了多项改进。我们基于 RDMA 开发了具有许多优化和 failover 支持的全新存储协议，并能在传统存储协议栈中进行无缝集成。我们构建了 RDMA Estats 工具用来监控主机网络协议栈的状态。我们通过 SONiC 在不同交换机平台上实施统一的软件栈部署。我们更新了 NIC 的固件以统一其 DCQCN 行为，并结合使用 PFC 和 DCQCN 来实现网络的高吞吐、低延迟和近乎零丢包。

![](/img/2024-01-03-empowering-azure-storage-with-rdma/figure1.png)
图 1：2023 年 1 月 18 日至 2 月 16 日期间 Azure 公有云所有 region 的流量统计数据。流量是通过收集所有 ToR 交换机上面向服务器一次的交换机端口计数器测量得到的，大约 70% 的流量是 RDMA 流量。

2018 年，我们开始为后端存储流量启用 RDMA。 2019 年，我们开始为客户前端存储流量启用 RDMA 。图 1 给出了 2023 年 1 月 18 日至 2 月 16 日期间 Azure 公有云所有 region 的流量统计数据。截至 2023 年 2 月，Azure 云中大约 70% 的流量是 RDMA 流量，并且 Azure 公有云所有 region 都支持 region 内 RDMA。 RDMA 帮助我们显着提高磁盘 I/O 性能并节​​省 CPU 资源。

## 2. 背景
- - -

在本节中，我们首先介绍 Azure 网络和存储架构的背景知识。然后，我们介绍一下 region 内实现 RDMA 网络的动机和挑战。

### 2.1 Azure region 的网络架构
- - -

![](/img/2024-01-03-empowering-azure-storage-with-rdma/figure2.png)

在云计算中，region 是部署在延迟定义的边界内的一组数据中心。图 2 显示了一个 Azure region 的简化拓扑。region 内的服务器通过基于以太网的 Clos 网络进行连接，该 Clos 网络具有四级（four tiers）交换架构：第 0 层（T0）、第 1 层（T1）、第 2 层（T2）和 region 枢纽层 (RH/region hub)。我们使用 eBGP 进行路由学习，使用 ECMP 进行负载平衡。
我们部署以下四种类型的组件：
* Rack（机架）：T0 交换机和连接到它的服务器。
* Cluster（集群）：连接到同一组 T1 交换机的一组机架。
* Datacenter（数据中心）：连接到同一组 T2 交换机的一组集群。
* Region（地域）：连接到同一组 RH 交换机的数据中心。与数据中心中的短链路（几米到数百米）相比，T2 和 RH 交换机通过长度可达数十公里的长距离链路进行连接。

该架构有两点需要注意：首先，由于 T2 和 RH 之间通过长距离链路进行连接，基本 RTT 从数据中心内的几微秒到 region 内的 2 毫秒进行变化。其次，我们使用两种类型的交换机：用于 T0 和 T1 的盒式交换机（pizza box switch），以及用于 T2 和 RH 的框式交换机（chassis switch）。盒式交换机已被学术界广泛研究并使用，该交换机通常带有一颗浅 buffer 的 ASIC 交换芯片。相比之下，框式交换机具有多颗基于虚拟输出队列 (VoQ) 架构的深 buffer ASIC 交换芯片。

### 2.2 Azure 存储的高层级架构
- - -

![](/img/2024-01-03-empowering-azure-storage-with-rdma/figure3.png)

在 Azure 云中，我们计算集群与存储资源进行分离以节省成本并支持自动扩展。在 Azure 云中主要有两种类型的集群：计算集群和存储集群。在计算集群中创建 VM ，但是其虚拟硬盘 (VHD) 实际存储在存储集群中。

图 3 显示了 Azure 云存储的高层级架构。 Azure 云存储分为三层：前端层（Front-End Layer）、分区层（Partition Layer）和文件流层（Stream Layer）。文件流层：是一个附加的分布式文件系统。在该层中信息 bit 位存储在磁盘上并进行复制存储以实现信息持久化，但在这一层它并不理解更高层级的存储抽象，例如 Blob、Table 和 VHD。分区层：负责理解不同的存储抽象，管理存储集群中的所有数据对象分区，并将对象数据存储在文件流层之上。分区层和文件流层的守护进程分别称为分区服务器（PS：Partition Server）和扩展节点（EN：Extent Node）。 PS 和 EN 共同位于每个存储服务器上。前端层（FE）由一组服务器组成，用于验证传入请求并将其转发到相应的 PS。在某些情况下，FE 服务器也可以直接访问文件流层以提高效率。

当 VM 想要对其磁盘进行写入时，运行在计算服务器主机域中的磁盘驱动程序向相应的存储集群发送 I/O 请求。 FE 或 PS 解析并验证请求，生成请求到相应的位于文件流层的 EN 去进行写入数据。在文件流层，文件本质上是称为“extern”的大型存储块（chunk）的有序列表。要写入文件，数据会附加到 active extern 的末尾，该 active extern 会在存储集群中被复制 3 份以确保数据的高可用。只有在收到所有主数据与副本数据的 EN 写入成功的响应后，FE 或 PS 才会将最终响应发送回磁盘驱动器。相比之下，磁盘读取则不同。FE 或 PS 从任何 EN 副本读取数据并将响应发送回磁盘驱动器。

除了面向用户的工作负载之外，存储集群中还存在许多后台工作负载，例如垃圾收集和纠删码。我们将存储流量分为两类：前端流量（VM 和存储服务器之间的流量，例如 VHD 写入和读取请求）和后端流量（存储服务器之间的流量，例如复制和磁盘重建）。我们的存储流量具有类似 incast 的特征。最典型的例子就是在文件流层中实现的数据重建。文件流层纠删码将一个密封的 extent 分割成若干个分片，然后将编码后的分片发送到不同的存储服务器进行存储。当用户想要读取的某个分片由于故障而无法获取时，文件流层会从多个存储服务器中读取其他分片来重建该目标分片。

### 2.3 Region 内开启 RDMA 的动机
- - -

近年来，存储技术有了显着的进步。例如，NVMe 的固态硬盘 (SSD) 可以在请求延迟为数百微秒的前提下提供数十 Gbps 的吞吐。许多客户要求在云中也要有类似的性能。由于云中存算分离（Disaggregated Storage）和分布式存储架构（Distributed Storage）的原因，高性能云存储解决方案对底层网络提出了严格的性能要求。虽然数据中心网络提供了足够的网络带宽，但操作系统内核中的传统 TCP/IP 协议栈由于其高处理延迟和低单核吞吐而成为性能瓶颈。更糟糕的是，传统 TCP/IP 协议栈的性能还取决于操作系统的调度。为了提供可预测的存储性能，我们必须在计算和存储节点上预留足够的 CPU，以便 TCP/IP 协议栈能够处理峰值时的存储工作负载。这些预留的 CPU 原本可以作为客户 VM 进行售卖，这增加了云服务的总体成本。

鉴于这些限制，通过 RDMA 提供了一个有效的解决方案。通过将网络协议栈卸载到网卡（NIC）硬件上，RDMA 在 CPU 接近零开销的情况下，实现了超低处理延迟（几微秒）和超高吞吐（单流线速）。除了性能优势之外，RDMA 还减少了每台服务器上为网络协议栈处理所预留的 CPU 个数。这些原本预留用作网络处理的 CPU 可以作为客户 VM 进行售卖或用于存储请求处理。

为了充分发挥 RDMA 的优势，我们为存储前端流量和后端流量都启用了 RDMA。为存储后端流量启用 RDMA 相对容易，因为几乎所有后端流量都在存储集群内部。相比之下，前端流量需要跨越 Region 内的不同集群。尽管我们尝试将相应的计算集群和存储集群放在同一位置以最大程度地减少延迟，但有时由于容量的原因，它们最终仍可能位于同一 region 内的不同 dc。这就要求我们必须在 region 规模内支持 RDMA，才能满足我们的存储工作负载的要求。

### 2.4 挑战
- - -

在 region 内启用 RDMA 时，我们面临许多挑战：

**现实考量：** 我们的目标是在现有 region 内基础设施上启用  RDMA。虽然我们可以灵活地重新配置和升级软件协议栈，例如网卡驱动、交换机操作系统和存储协议栈，但是更换底层硬件（例如网卡和交换机）是不可行的。因此，为了保持与 IP 路由网络的兼容性，我们采用了基于 RoCEv2 的 RDMA 技术。在开始这个项目之前，我们已经部署了大量的第一代 RDMA NIC，它们在 NIC 固件中实现了 go-back-N 重传。我们的测量数据表明，该固件丢包恢复时间需要数百微秒，这甚至比 TCP/IP 协议栈还要糟糕。鉴于如此之大的性能下降，我们决定采用基于 PFC 的流控来消除由于网络拥塞导致的报文丢失。

**挑战：** 在此项目之前，我们已经在一些集群中部署了 RDMA 以支持 Bing 服务，我们从这次部署中吸取了一些教训。与在一个集群内部署 RDMA 相比，region 范围内部署 RDMA 由于基础设施的高度复杂性和异构性，引入了许多新的挑战。

* NIC 差异：云基础设施不断发展，通常最新一代的服务器一次一个集群或一个机架进行部署。region 内的不同集群可能使用不同的 NIC。我们的部署的服务器包含三代 RDMA NIC：Gen1、Gen2 和 Gen3。每代 NIC 的 DCQCN 都有不同实现方式。当具有不同代差的 NIC 相互通信时，这会导致许多无法预期的行为。

* 交换机差异：与服务器基础设施类似，我们部署新一代交换机以降低运营成本并增加网络带宽。我们部署了来自多个不同供应商的多款 ASIC 交换机和多种交换机操作系统。由于诸如：buffer 大小、分配机制、监控和配置等许多方面都是厂商绑定的，这显着增加了我们的运营工作量。

* 延迟差异：如 2.1 所示，由于 T2 和 RH 之间的长距离链路，一个 region 内的 RTT 变化很大，从几微秒到2毫秒不等。因此，RTT 的公平性成为了一个关键挑战。此外，长距离链路带来的较大传播时延也对 PFC headroom 造成了较大压力。

与公共云中的其他服务一样，可靠性（availability）、可维护性（diagnosis）和可用性（serviceability）是我们的 RDMA 存储系统的关键指标。为了实现高可用，尽管我们已在测试方面投入了大量资金，但我们始终为可能突发的 0day 问题做好准备。我们的系统必须检测设备的性能异常状况并在必要时执行自动故障转移。为了了解设备故障状况和进行故障调试，我们必须构建细粒度的遥测系统，以便于为端到端路径中的每个组件提供网络转发的可视化（clear visibility）。我们的系统还必须具备可维护性：在设备 NIC 驱动更新和交换机软件更新后能够继续承载存储工作负载。

## 3. 概述
- - -

为了安全地在 region 内为 Azure 存储流量启用 RDMA，我们从应用层协议到链路层流量控制等方面对网络基础设施进行了多项改进。我们开发了两种基于 RDMA 的协议：sU-RDMA 和 sK-RDMA，分别用于支持存储后端通信和存储前端通信，并将其无缝集成到传统存储协议栈中。在存储协议和 NIC 之间，我们部署了一个监控系统 RDMA Estats，它能使我们了解主机网络协议栈为每次 RDMA 操作所消耗的代价。

我们通过 PFC 和 DCQCN 的组合来实现高吞吐、低延迟的网络，即是在拥塞情况下也能带来近乎零丢包网络体验。在启动该项目时，DCQCN 和 PFC 是当时最先进的商业解决方案。为了优化用户体验，我们使用两个优先级来隔离存储前端流量和存储后端流量。为了解决交换机的差异性问题，我们开发并部署了 SONiC 系统，以提供跨交换机平台的统一软件栈。为了解决不同类型 NIC 的互操作性问题，我们更新了 NIC 的固件以统一其 DCQCN 行为。我们仔细调整了 DCQCN 和交换机的 buffer 参数，以优化不同场景下的网络性能。

### 3.1 使用 Watchdog 缓解 PFC 风暴
- - -

我们通过 PFC 来防止拥塞丢包。然而，发生故障的 NIC 和交换机可能会在没有发生拥塞的情况下持续发送 PFC 暂停帧，从而导致长时间完全阻塞对端设备。此外，这些无休止的 PFC 暂停帧最终会传播到整个转发网络，从而对无辜设备造成附带损害。这种无休止的 PFC 暂停帧被称为 PFC 风暴。相反，正常由于拥塞而触发的 PFC 暂停帧仅通过间歇性暂停和恢复减慢对端设备的数据传输速率。

为了检测和缓解 PFC 风暴，我们在 T0 交换机和服务器之间的每台交换机和 FPGA 卡上设计并部署了 PFC 看门狗。当 PFC 看门狗检测到某个队列处于暂停状态的时间异常长时（例如数百毫秒），它会禁用 PFC 并丢弃该队列上的所有数据包，从而防止 PFC 风暴传播到整个网络。

### 3.2 安全
- - -

我们使用 RDMA 在可信环境中支持第一方（first-party）的存储流量，包括存储服务器、计算服务器的主机域、交换机和链路。

## 4. 基于 RDMA 的存储协议
- - -

在本节中，我们介绍两种构建在 RDMA 可靠连接 (RC) 之上的存储协议：sU-RDMA 和 sK-RDMA。这两种协议都旨在优化性能的同时保证了与传统协议栈的良好兼容性。

### 4.1 sU-RDMA
- - -

![](/img/2024-01-03-empowering-azure-storage-with-rdma/figure4.png)

sU-RDMA 用于存储后端（存储到存储）通信。图 4 展示了我们存储后端网络协议栈架构。 Azure 存储网络协议是应用程序可以直接使用的 RPC 协议，应用程序可以直接使用它发送请求和接收响应。它使用 socket API 来实现连接管理、消息的发送和接收。

为了简化 RDMA 与存储协议栈的集成难度，我们构建了用户空间的 sU-RDMA Lib 库，向上层应用公开类似于 socket 的字节流 API。为了将类似的 socket API 映射到 RDMA 操作，sU-RDMA Lib 需要应对以下挑战：
* 当 RDMA 应用无法直接向现有 MR（Memory Region）写入数据时，它必须将应用程序的 buffer 注册为新的 MR 或将其数据复制到现有 MR 中。这两种操作都会带来巨大的延迟，我们应该尽量减少这类开销。
* 如果我们使用 RDMA 发送和接收数据，接收方必须预先发布足够的接收请求。
* RDMA 发送方和接收方必须就传输的数据大小达成一致。

为了减少内存注册（这对于小消息来说尤为重要），sU-RDMA Lib 维护一个基于预注册内存机制的跨多个连接共享的公共 buffer 池。sU-RDMA Lib 还提供 API 来允许应用程序请求和释放已注册的缓冲区。为了避免 NIC 上的 MTT（Memory Translation Table）缓存 miss，sU-RDMA Lib 从内核中分配大内存块，并在这些块上注册内存。该缓存池还可以根据运行时的使用情况进行自动缩放。为了避免淹没（overwhelming）接收方，sU-RDMA Lib 实现了基于信用的 receiver 流制驱动，其中信用代表 receiver 分配的资源（例如，可用缓冲区和发布的接收请求）。接收方定期将信用更新消息发送回发送方。当我们开始设计 sU-RDMA Lib 时，我们考虑为每个 RDMA 传输数据的发送/接收请求使用大小为 S 的固定缓冲区。然而，这种设计却带来一个问题。如果我们使用一个较大的 S 缓冲区，我们可能会浪费很多内存空间，因为无论实际消息大小如何，发送请求都会完全使用接收请求的接收缓冲区。相反，较小的 S 缓冲区会造成较大的数据碎片。因此，sU-RDMA Lib 根据消息大小使用三种传输模式。

* 小消息：使用 RDMA 发送和接收传输数据。
* 中消息：发送方通过发布 RDMA 写入请求以传输数据，并发送带有“写入完成”的发送请求以通知接收方。
* 大消息：发送方首先向接收方发布携带本地数据缓冲区描述的 RDMA 发送请求。然后接收方发出读取请求来拉取数据。最后，接收方发布带有“Read Done”的发送请求来通知发送方。

在 sU-RDMA Lib 之上，我们构建模块来实现 TCP 和 RDMA 之间的动态转换，这对于故障转移和业务恢复来说至关重要。过渡过程是渐进式的，我们定期关闭所有连接的一小部分并使用所需的传输方式建立新的连接。

与 TCP 使用跟踪传输中的数据包数量（窗口大小）的拥塞控制算法不同，RDMA 使用基于速率的拥塞控制算法。因此，RDMA 往往会由于发送了过多的数据包，从而触发 PFC。为了缓解这个问题，我们在 Azure 存储网络协议中实现了静态流量控制机制，将消息划分为固定（fixed-sized）大小的块（chunk），并且每个连接只允许一个正在发送（in-flight）的块。分块可以在 CPU 开销可以忽略不计的条件下显着的提高 incast 场景下的性能。

### 4.2 sK-RDMA
- - -
sK-RDMA 用于存储前端（计算集群到存储集群之间）的通信。与通过 sU-RDMA 在用户空间运行 RDMA 相比，sK-RDMA 在内核空间运行 RDMA。这使得在计算服务器主机域内核空间中运行的磁盘驱动能够直接使用 sK-RDMA 发出网络 I/O 请求。sK-RDMA 直接通过服务器消息块 (SMB：Server Message Block) 提供类似套接字的内核模式 RDMA 接口。与 sU-RDMA 类似，sK-RDMA 也提供了基于信用的流量控制以及 RDMA 和 TCP 之间的动态转换。

![](/img/2024-01-03-empowering-azure-storage-with-rdma/figure5.png)

图 5 显示了 sK-RDMA 读写磁盘的数据流过程。计算服务器首先向注册数据缓冲区发布快速内存注册（FMR）请求。然后，发布 RDMA 发送请求以将请求消息传送到存储服务器。该请求携带磁盘 I/O 命令以及可用于 RDMA 访问的 FMR 注册缓冲区的描述。根据 InfiniBand (IB) 规范，NIC 应等待 FMR 请求完成，然后再处理任何后续发布的请求。因此，请求消息实际上是在内存注​​册之后才被推送到链路上。数据传输由存储服务器使用 RDMA 读取或写入发起。数据传输后，存储服务器使用 RDMA Send With Invalidate 向计算服务器发送响应消息。

为了检测由于路径上的各种软件和硬件错误而发生的数据损坏，sK-RDMA 和 sU-RDMA 都对所有应用数据实施 CRC 校验。在 sK-RDMA 中，计算服务器计算磁盘写入数据的 CRC。这些计算出的 CRC 包含在请求消息中，并由存储服务器用来验证数据。对于磁盘读取，存储服务器执行 CRC 计算并将其包含在响应消息中，计算服务器使用它来校验数据。

## 5. RDMA Estats
- - -

为了理解和调试故障，我们需要细粒度的遥测工具来捕获端到端路径中每个组件的行为。尽管有许多现有工具[51,97,114]来诊断交换机和链路故障，但这些工具都没有让我们能够很好地了解终端主机上的 RDMA 网络堆栈。

受 TCP 诊断工具 [79] 的启发，我们开发了 RDMA 扩展统计（Estats）来诊断网络和主机的性能问题。如果 RDMA 应用程序性能不佳，RDMA Estats 使我们能够判断瓶颈是在发送方、接收方还是网络中。

为此，RDMA Estats 除了收集常规计数器（例如发送/接收的字节数和 NACK 数量）之外，还为每个 RDMA 操作提供细粒度的延迟细分。当工作队列元素 (WQE) 遍历传输管道时，请求者 NIC 会记录一个或多个测量点处的时间戳。当收到响应（ACK 或读取响应）时，NIC 会在接收管道沿线的测量点记录附加时间戳（图 6）。 Azure 中的任何 RDMA Estats 实施都需要以下测量点

T1：WQE 发布：WQE 发布到提交队列时的主机处理器时间戳。 T5：CQE 生成：NIC 中生成完成队列元素（CQE）时的 NIC 时间戳。 T6：CQE轮询：软件轮询CQE时的主机时间戳。

在 Azure 中，NIC 驱动程序报告源自上述时间戳的各种延迟。例如，T6 - T1 是 RDMA 消费者看到的操作延迟，而 T5 - T1 是 NIC 看到的延迟。用户模式代理按连接、操作类型和（成功/失败）状态对延迟样本进行分组，以便为​​每个组创建延迟直方图。默认情况下，直方图涵盖一分钟的间隔。每个直方图的分位数和摘要统计数据都会输入到 Azure 的遥测管道中。随着诊断技术的发展，我们在用户模式代理中添加了在高延迟事件期间收集和上传 NIC 和 QP 状态转储的功能。最后，我们扩展了用户模式代理的事件触发数据收集范围，以包括 NIC 统计数据和状态转储，以防发生非特定于 RDMA 的事件（例如，影响连接的服务操作）。

延迟样本的收集增加了 WQE 发布和完成处理代码路径的开销。此开销主要是保持 NIC 和主机时间戳同步。为了减少开销，我们开发了一种时钟同步程序，试图最大限度地降低读取 NIC 时钟寄存器的频率，同时保持较低的偏差。

RDMA Estats 可以通过快速排除（或消除）网络延迟来显着减少调试时间并缓解存储性能事件。在第 8.3 节中，我们分享了使用 RDMA Estats 诊断 FMR 隐藏栅栏错误的经验。

## 6. 交换机管理
- - -

### 6.1 通过 SONic 屏蔽交换机的厂商差异
- - -




## 参考
- - -
* [Empowering Azure Storage with RDMA](https://www.usenix.org/conference/nsdi23/presentation/bai)

## 公众号：Flowlet
- - -

<img src="/img/qrcode_flowlet.jpg" width = 30% height = 30% alt="Flowlet" align=center/>

- - -