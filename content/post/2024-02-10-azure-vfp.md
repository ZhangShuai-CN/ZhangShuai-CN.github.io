---
layout:     post
title:      "VFP：公有云主机 SDN 的虚拟交换平台 [NSDI'17]"
subtitle:   "公有云主机 SDN 的虚拟交换平台"
description: "VFP: A Virtual Switch Platform for Host SDN in the Public Cloud"
excerpt: ""
date:       2024-02-10 01:01:01
author:     "张帅"
image: "/img/2024-02-10-azure-vfp/background.jpg"
showtoc: true
draft: true
tags:
    - vSwitch
categories: [ Tech ]
URL: "/2024/02/10/azure-vfp/"
---

- - -
###### 关于作者
> 
> **`张帅，网络从业人员，公众号：Flowlet`**
> 
> **`个人博客：https://flowlet.net/`**
- - -

## 序言
- - -

公有云的 Host vSwitch 由于涉及的业务场景复杂（几乎所有的业务场景都跟 vSwitch 有关，例如：VPC、Nat、EIP、LB、NFV、容器场景等等），配置变更频繁，性能要求极高。同时由于国内公有云业务极其内卷，Iaas 服务基本上就是比拼价格，事实上已经沦为卖铁。VM 超卖严重，分配给 Host 用于网络处理的 CPU 与内存资源极其有限。

Host vSwitch 的设计就如同带着镣铐跳舞，基于开源的 Open vSwitch 并不能满足超大规模云网络的技术指标（单 Region 支持百万级 VPC，单 VPC 支持百万级计算节点）要求，因此国内的 Top 级互联网云厂商 Host vSwitch 都采用了自研架构。从 0 到 1 设计 vSwitch 需要熟悉所有的公有云网络业务场景，并且需要极强的技术功底与网络架构能力。

本文翻译自 NSDI'17 论文《[VFP: A Virtual Switch Platform for Host SDN in the Public Cloud](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf)》，该文章介绍了微软 Azure 云在设计 Host vSwitch（VFP：Virtual Filtering Platform）方面的经验。

## 前言
- - -

由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。

## 摘要
- - -

许多现代可扩展云网络架构都依赖主机网络来实施 VM 网络策略 ———— 例如，用于虚拟网络的隧道、用于负载均衡的 NAT、带状态的 ACL、QoS 等。我们推出虚拟过滤平台 (VFP：Virtual Filtering Platform) ———— 为 Microsoft Azure（大型公共云）提供该网络策略支持的一种可编程虚拟交换机。根据我们的运营经验我们为可编程虚拟交换机定下了以下几个主要目标，包括支持多个独立的网络控制器、基于连接而不是仅仅基于数据包的网络策略、通过高效缓存和分类算法来提高性能以及将流策略高效的卸载到可编程 NIC，并演示 VFP 是如何实现这些目标的。VFP 已部署在超过 100 万台运行 IaaS 和 PaaS 工作负载的主机上超过 4 年。我们将介绍 VFP 及其 API 设计、用于流处理的流语言和编译器、性能结果以及多年来在 Azure 中部署和使用 VFP 的经验。

## 1. 介绍
- - -

Amazon Web Services（AWS）、Microsoft Azure（Azure） 和 Google Cloud Platform（GCP） 等公有云工作的兴起造就了超大规模的数据中心，根据云服务供应商的公开资料显示服务器数量已经达到数百万台。这些云服务供应商不仅要为客户提供弹性伸缩和高性能的虚拟机（VM），还必须提供丰富的网络语义，例如，为客户提供的独立网络地址空间的专用虚拟网络、可扩展的四层 LB、安全组和 ACL 、虚拟路由表、限速（bandwidth metering）、QoS 等等。

这些策略非常复杂，通常无法非常经济地在传统硬件核心路由器上大规模部署。相反，一种常见的方法是在 VM 所在主机上的软件中实施这些策略（在将 VM 连接到外部网络的虚拟交换机 (vswitch) 中实施），该方法可以随着服务器数量而扩展，并使得物理网络简单、可扩展。由于该模型将集中式控制面与主机上的数据面分开，因此它被广泛认为是软件定义网络 (SDN) 的一个示例，特别是基于主机的 SDN。

作为大型公共云服务供应商，Azure 基于主机 SDN 构建其云网络，使用主机 SDN 来实现我们所提供的几乎所有的虚拟网络功能。近年来围绕 SDN 的大部分关注焦点一直都集中在致力于构建可扩展且灵活的网络控制器，虽然这一点至关重要，但是可编程 vswitch 的设计同样重要。因为云中的工作负载对成本和性能十分敏感，因此高度可编程数据面具有高性能和低开销的要求，高性能与低开销本身就是相互冲突的两个需求。

在本文中，我们提出虚拟过滤平台（VFP：Virtual Filtering Platform）———— 我们在所有主机上运行的云级虚拟交换机。VFP 之所以如此命名，是因为它充当 VM 上每个虚拟 NIC 的过滤引擎，允许控制器对其 SDN 策略进行编程。我们的目标是展示 VFP 的设计思路以及在大规模生产环境中运行 VFP 的经验。

### 1.1 相关工作
- - -

在本文中，我们通过文献中的两个例子，来展示如何通过 VFP 支持示例中的策略。第一个示例是 VL2，它通过使用主机间的无状态隧道创建虚拟网络 (VNET)。第二个示例是 Ananta，一个可扩展的 4 层 LB（Layer-4 load
balancer），它通过在终端主机的 vswitch 中运行 LB-NAT（load balancing NAT）来进行扩展，使网络内的 LB 无状态且可扩展。

此外，我们还对 OpenFlow 和 OpenVswitch（实现 OpenFlow 协议的开源 vswitch ）进行了参考比较，这是 SDN 领域的两个开创性的项目。我们从公有云的角度指出了两者之间的核心设计差异，我们希望能在更广泛的社区分享这些经验教训。

## 2. 设计目标与基本原理
- - -

VFP 的设计根据我们运行大型公有云平台的经验随着时间而不断的发展。VFP 并不是我们最初的 vswitch，在主机网络中实现 vswitch 也不是什么开创性的想法（早在 VL2 和 Ananta 中就已经提出了在主机网络中实现 vswitch 的想法）。

最初，我们在 Windows Hyper-V Hpervisor（虚拟机管理程序）之上为每个主机功能构建了网络过滤驱动（用于 ACL 的状态防火墙驱动、用于 VL2 VNET 的隧道驱动、用于 Ananta LB 的 NAT 驱动、 QoS 驱动等），并将其链接到 vswitch 中。随着主机网络成为我们虚拟化策略的主要工具，在得出为主机网络功能新构建固定过滤驱动程序不可扩展的结论后，我们决定在 2011 年创建 VFP。相反，我们创建了一个基于匹配动作表（MAT：Match-Action Table）模型的单一平台。这就是我们针对 VFP client 而设计的 VFP API 编程模型的由来。

VFP 的核心设计思想是从构建和运行过滤器（filter）以及在这些过滤器之上的控制器（network controllers）和代理（agent）的经验中得到的。

### 2.1 最初的目标
- - -

以下是 VFP 项目创立之初的目标：

> 1. 提供一个同时允许多个独立的网络控制器对网络应用进行编程的编程模型，最大限度地减少跨控制器的依赖性。

OpenFlow 和与此类似的 MAT 模型通常假设单一分布式网络控制器负责对交换机进行编程（也可能从其他控制器中获得输入）。我们的经验是，这种模型并不适合云中 SDN 的开发 ———— 相反，相互独立的开发团队经常为这些不同的网络应用构建不同的控制器和 agent。与通过向现有控制器添加逻辑相比，该模型减少了复杂的依赖关系，可以提供更好地可扩展性（scales better）和可维护性（more serviceable）。我们需要有一种设计，不仅允许控制器能够独立创建流表并对其进行编程，还要它们之间能够有良好的分层和边界（例如，不允许规则像 OpenFlow 中的那样随意的 GOTO 到其他表），以便可以通过开发新控制器来添加新功能，无需考虑旧控制器的行为逻辑。

> 2. 提供一个能够使用连接（带状态的规则）而不仅仅是数据包作为基本原语的 MAT 编程模型。

OpenFlow 的原始 MAT 模型历史上源自可编程交换机或路由器的 ASIC，因此假设由于硬件资源有限，数据包分类必须是无状态的。然而，我们发现我们的控制器需要基于连接，而不仅仅是基于数据包的策略 ———— 例如，最终用户经常发现通过使用有状态的访问控制列表（ACL）（例如，允许出站连接，但不允许入站连接）比在商用交换机中使用无状态的 ACL 来保护其虚拟机安全更加有用。控制器同样还需要 NAT（例如，Ananta）和其他有状态的策略。在软件 vswitch 中比在 ASIC 中更容易实现有状态的策略处理，我们的 MAT 模型应该利用这一点优势。

> 3. 提供一个允许控制器自定义自己的策略（policy）和动作（action）的编程模型，而不仅仅是提供只针对预定义场景的固定策略集。

由于 OpenFlow 提供的 MAT 模型的局限性（一组有限的 aciton 集、rule 有限的可扩展性、无表类型），OpenFlow 交换机（例如，OVS）在 MAT 模型之外添加了虚拟化功能。例如，通过 OVSDB 中的 VTEP（虚拟隧道端点：virtual tunnel endpoint）模式构建虚拟网络，而不是通过规则指定要封装（encap）和解封装（decap）哪些数据包以及如何执行此操作。

相反，我们更喜欢所有功能都基于 MAT 模型实现，并尝试将尽可能多的业务逻辑上移到控制器，vswitch 只保留核心的数据面逻辑。例如，可以通过匹配适当条件的可编程封装和解封规则来实现 VNET，而不是通过定义什么是 VNET 的模式，将 VNET 定义保留在控制器中，这大大减少了每次 VNET 定义发生变化时都需要不断对数据面进行扩展的次数。

P4 语言尝试为交换机或 vswitch 实现类似的目标，P4 语言非常的通用，例如，允许动态（on the fly）定义新的报文头。由于我们更新 vswitch 的频率远高于在网络中定义新报文头的频率，因此我们更喜欢预编译形式的快速报文头解析库，以及面向有状态连接处理而构建的编程语言。

### 2.2 在生产环境中学到的目标
- - -

根据 VFP 最初部署的经验教训，我们对 VFPv2 增加了以下目标（这是 13 到 14 年里的一次重大更新，该目标主要围绕可维护性（serviceability）和性能两方面）：

> 4. 提供允许无需重启或中断虚拟机状态流连接即可实现频繁部署和更新的可维护模型以及提供强大的服务监控能力。

随着我们的规模急剧增长（dramatically）（主机规模从 O(10K) 到 O(1M) ），随着越来越多的控制器基于 VFP 构建，以及越来越多的工程师加入，我们对频繁更新（功能和错误修复）的需求比以往任何时候都多。在基础设施即服务（IaaS：Infrastructure as a Servic）模型中，我们还发现客户不能容忍对单个 VM 进行停机更新。使用我们的有状态流模型（stateful flow model）来实现这一目标更具有挑战性，尤其是在跨版本更新（across update）时更不容易。

> 5. 通过大量的缓存（extensive cache）设计，在即使存在大量表项和规则的情况下，也能够提供非常高的包速率。

随着时间的推移，我们发现越来越多的控制器通过主机 SDN 模型进行构建，主机 SDN 变得越来越流行。很快我们就部署了大量流表（10+），每张流表都有许多规则，因为数据包必须遍历每张表，这大大降低了转发性能。与此同时，主机上的 VM 密度不断增加，促使我们网卡速率逐步从 1G 到 10G 再到 40G，甚至更高速率。我们需要找到一种在不影响性能的情况下能够容纳更多策略的方法，并得出结论，我们需要跨表执行流动作（flow action）的编辑（compilation），并使用大量的流缓存设计，以便现有流上的数据包能够与预编辑的动作（precompiled action）相匹配，而不必遍历表。`针对具有大量规则和表的情况提供快速数据包分类算法。`

虽然目标 #5 显着的提升了现有流的性能（例如，SYN 报文后的所有 TCP 报文），但我们发现一些应用将数千条规则推送到其流表中（例如，BGP peer 是客户的分布式路由器，使用 VFP 作为它的 FIB 表），这减慢了我们的流编译器的速度。我们需要设计一个高效的数据包分类器来处理该情况下遇到的性能问题。

> 6. 实施一种将流策略卸载到可编程 NIC 的有效机制，无需考虑（assuming）复杂规则的处理。

当我们使用 40G+ NIC 时，我们希望将策略卸载到 NIC 以支持 SR-IOV 场景，让 NIC 对相关 VM 的数据包直接执行相关的 VFP 策略。然而，随着控制器创建许多具有更多规则的流表，我们得出的结论是，直接卸载这些表 服务器 NIC 需要付出极其昂贵的硬件资源（例如，大 TCAM 表、串联匹配）。因此，对比将分类操作进行 offload，我们更想得到一个能够与预编辑精确匹配流很好配合的卸载模型，要求硬件只支持访问 DRAM 中的大缓存流表，并支持我们相关的动作语义。

### 2.3 非目标
- - -

以下是我们在其他项目中看到的目标，根据我们的经验，我们选择不支持（pursue）这些目标：

> 1. 跨平台可移植。

在内核的高性能数据路径中很难实现跨平台移植。 OVS 等项目通过将其分为内核态快速路径和可移植的用户态慢速路径来实现此目的，但这样做的代价是，当数据包采用慢速路径时，速率会降低一个数量级。我们只在一种主机操作系统上运行，所以跨平台可移植不是我们的目标。

> 2. VFP 自身支持远程配置协议。

OpenFlow 既包含网络编程模型，也包含通过远程配置协议。OVS 和 OVSDB 协议也是如此。为了支持管理策略不同的控制器模型（例如，规则推送（push）模型或 VL2 目录系统拉取（pull）模型），我们将 VFP 解耦为 vswitch 与实现远程配置协议的 agent，该 agent 专注于提供高性能的 host API。

> 3. 提供检测或防止控制器编程策略冲突的机制

许多文献描述了检测或防止策略在流表或规则匹配系统中的冲突方法。尽管我们的第一个目标就是支持多个控制器对 VFP 进行并行编程而不会互相干扰，但我们很早就得出结论，由于种种原因，显式的进行冲突管理既不可行也非必要。对 VFP 进行编程是一项受保护的操作，只有我们的控制器才能执行，因此我们不必担心恶意控制器。此外，我们得出的结论是，不可能区分错误编程（misprogrammed）的流表（意外的覆盖另一个流表的动作）和一个流表被设计用于过滤另一个表的输出，它们之间的区别。相反，我们专注于开发用于帮助开发人员验证其策略的工具。

## 3. 概述与比较
- - -

本文的第一个示例，我们考虑一个简单的场景，需要 4 个用于云中 O(1M) 个 VM 的 Host 策略。每个策略均由其自己的 SDN 控制器进行编程，并且需要高性能和支持 SR-IOV offload：VL2 型 VNET、Ananta 型 LB、带状态防火墙以及用于计费目的的目的地（per-destination）流量统计。我们首先根据现有解决方案对此进行评估，以证明需要我们所描述的不同方法。第 4-7 节将详细介绍 VFP 的核心设计。

### 3.1 现有解决方案：Open vSwitch
- - -

虽然 Linux 和 Windows 支持将多个接口进行桥接（可用作 vswitch），但这种网桥不适用于 SDN 策略。其他公共云厂商（例如，Google）描述了其如何使用主机 SDN 策略，但详细信息并未公开。 OVS 是当今提供基于 vswitch SDN 的主要解决方案，因此其是我们的主要比较对象。

我们相信 OVS 在让可编程主机网络方面有着巨大的影响力。OVS 的许多设计都是由于 OVS 特定目标（例如，支持跨平台，并随 Linux 内核一起发布）导致的。结合 OVS 对 OpenFlow 的使用，这么设计的目的是控制器能够通过相同的协议同时管理虚拟交换机和物理交换机，但这并不是我们主机网络模型的目标。OVS 还支持许多对物理交换机有用的协议（例如 STP、SPBM、BFD 和 IGMP Snooping），但我们并不使用这些协议。

OpenFlow 与 OVS 在某些方面并不适用于我们的工作场景：

* OVS 本身并不支持真正的多控制器模型，而当我们的 VL2 和 Ananta 应用需要单独控制时则需要这种模型。OpenFlow 的底层表模型并不适用于多控制器场景 ———— 表规则显示的指定 GOTO 到对下一张表，导致控制器将这些表的策略绑定在一起。另外，表只能正向遍历，在多控制器的场景下要求出站报文与入站报文相反的方向遍历表，以便报文在任一方向匹配该控制器策略时都处于一致的状态。VFP 通过显式的将表进行分层解决这个问题（§5.2）。

* OVS 本身在其 MAT 模型中并不支持 NAT 之类的有状态 action，而我们的 Ananta 示例需要这种操作（我们的防火墙也是有状态的）———— 在这两种情况下，控制器需要将连接（而不是数据包）作为基本操作原语。然而，OpenFlow 仅提供数据包模型。 OVS 最近新添加了将数据包发送到 Linux 连接跟踪以支持有状态防火墙，但它没有作为 MAT 模型进行公开，并且并不容易支持 NAT，这需要显式的双向有状态表，以便 NAT 在流的返回路径上进行反转。VFP 通过状态层解决这个问题（§5.2）。

* OVS 的 VTEP 模式（schema）需要通过显式隧道接口来实现 VL2 风格的 VNET，而不是允许控制器指定自己的 encap/decap action（OpenFlow 本身并不支持这种类型的 action）。在数据面中硬编码 VNET 模型，而不是允许控制器定义 VNET 如何工作（目标 3）。在此模式中添加复杂的 VNET 逻辑（例如，ECMP 路由）可能很困难，并且需要对 vswitch 进行修改，而不仅仅是更改策略。 VFP 通过将 encap/decap 建模为 action，直接在其 MAT (§5.3) 中就能支持。

* OVS 不支持动态查找客户地址到物理地址映射所需的 VL2 样式目录系统。 OpenFlow 的设计在以这种方式支持大型 VNET 方面缺乏可扩展性 ———— OpenFlow 的异常报文必须全部返回到中央控制器，并且在 OVS 中，所有主机上的 VTEP 都应在映射发生变化时进行更新。这对于支持最多 1000 个主机的 NSX/vSphere 来说是没问题的，但我们发现这在公有云这个规模量级上是不可用的。 VFP 通过将无模式（schema-free） MAT 模型与高效的异步I/O 异常请求（§5.5.1）相结合来解决这一问题，agent 可以将这些请求重定向到与控制器分离的处理模块。

* OVS 没有通用的卸载 action 语言或 API 来支持策略组合（例如，Ananta NAT 加 VL2 encap）。虽然 NIC 供应商已在 OVS 构建之上针对特定工作负载（例如，VTEP 模式）实施了 SR-IOV 卸载，但执行通用卸载需要硬件支持原始策略的复杂多表查找，我们发现其在实践中成本相当高。VFP 的协议头转换语言（Header Transposition language）（§6.1.2, 9.3）只需在硬件中进行一次表查找即可为所有策略提供 SR-IOV 支持。

因此，我们的政策需要不同的设计。

### 3.2 VFP 设计
- - -

![](/img/2024-02-10-azure-vfp/figure1.jpg)

图 1 展示了 VFP 的设计模型，后续部分将对此进行描述。VFP 在 Hyper-V 可扩展交换机（Hyper-V extensible switch）之上运行，如第 4 节过滤模型中所述。VFP 将 MAT 作为支持多控制器模型的层来实现，编程模型在第 5 节中介绍。第 6 节描述了 VFP 的包处理器，包括通过统一流表的快速路径以及用于匹配 MAT 层中的规则的分类器。第 7 节介绍了 VFP 桥的交换模型。

## 4. 过滤模型（Filtering Model）
- - -

VFP 通过 MAT 流表策略对操作系统中经过的报文进行过滤。下面介绍过滤模型：

### 4.1 端口与网卡（Ports and NICs）
- - -

核心 VFP 模型假设 switch 具有多个连接到虚拟 NIC（VNIC）的端口。VFP 过滤从 VNIC 到 switch 以及从 switch 到 VNIC 的流量。所有 VFP 策略都附属（attached to）于特定的端口。从 VM 的角度来看（具有连接到端口的 VNIC），switch 的 ingress 流量被认为来自 VM 的 outbound 流量，switch 的 egress 流量则被视为 VM 的 inbound 流量。 VFP API 及其策略基于 inbound/outbound 模型。

![](/img/2024-02-10-azure-vfp/figure2.jpg)

VFP 实现了一个 switch 抽象接口来抽象出不同的环境，其实例化提供了 port、VNIC 和关联对象的管理逻辑（例如，创建/删除/连接/断开连接）。该接口支持 Hyper-V switch 和本机主机过滤器，如图 2 所示。

### 4.2 Hyper-V switch 可扩展性
- - -

Hyper-V 包括一个基本的 vswitch，用于将 VNIC 桥接到物理 NIC。该 switch 是可扩展的，允许插入过滤器并过滤进出 VNIC 的流量。 VFP 充当 Hyper-V vswitch 的转发扩展 ———— 它只是通过自身简单的替换整个 switch 逻辑。使用此模型使我们能够将策略模块和 VFP 与 Hyper-V 基础设施中分开，以将报文传送到 VM 或从 VM 发送报文，从而提高其模块化和可维护性。

这种模式下的 VFP 支持 PacketDirect，它允许 client 以非常低的开销轮询 NIC。

## 5. 编程模型（Programming Model）
- - -

VFP 的核心编程模型基于 VFP 对象的层次结构，控制器可以创建这些对象并进行编程以指定其 SDN 策略。这些对象是：
* 端口，VFP策略过滤的基本单位。
* 层，保存 MAT 策略的有状态流表。
* 组，用于管理和控制层内相关规则组的实体。
* 规则，匹配操作表条目本身。

![](/img/2024-02-10-azure-vfp/figure3.jpg)

### 5.1 端口
- - -

VFP 的策略是在每个端口的基础上实施的 - 每个端口都有匹配操作表，可以位于端口的入站或出站路径上，充当过滤器。由于我们的控制器通常希望代表 VM 或 VNIC 对策略进行编程，因此这种干净的端口分离允许控制器独立管理不同 VM 上的策略，并仅在需要的端口上实例化和管理流表 - 例如，虚拟网络可能具有将流量封装和解封装到隧道中的表，而不在虚拟网络中的另一个 VM 不需要这些表（VNET 控制器甚至可能不知道它不需要管理的另一个 VM）。

VFP 上的策略对象按固定的对象层次结构排列，用于指定给定 API 调用引用哪个对象，例如层/组/规则。所有对象都使用优先级值进行编程，规则匹配将按照该顺序处理它们。

### 5.2 Layers
- - -

VFP 将端口的策略分为几层。层是控制器用来指定其策略的基本匹配操作表。它们可以由不同的控制器单独创建和管理，或者一个控制器可以创建多个层。每层都包含可以过滤和修改数据包的入站和出站规则和策略。从逻辑上讲，数据包逐层通过每一层，根据上一层执行操作后数据包的状态来匹配每层中的规则。控制器可以指定端口管道中各层相对于其他层的顺序，并在操作期间动态创建和销毁层。

![](/img/2024-02-10-azure-vfp/figure4.jpg)

重要的是，数据包在入站时和出站时以相反的顺序遍历各层。当控制器在层的任一侧实施相反的策略时，这会给它们带来“分层”效果。以实施 Ananta NAT 设计的负载平衡层为例。

在入站方向上，层 NAT 将目标虚拟 IP (VIP) 连接到 VIP 后面的直接 IP (DIP)，在本例中为虚拟机的 IP。在出站方向，它将数据包从 DIP NAT 回 VIP。因此，该层实现了地址空间边界——其上方的所有数据包都在“DIP 空间”中，而其下方的所有数据包都在“VIP 空间”中。其他控制器可以选择在此 NAT 层之上或之下创建层，并且可以分别探查规则以匹配 VIP 或 DIP - 所有这些都无需与 NAT 控制器协调或参与。

图 4 显示了我们的 SDN 部署示例的层。 VL2 由虚拟网络控制器编程的 VNET 层实现，使用客户地址 (CA) 隧道，以便数据包可以在虚拟机之间路径中的物理交换机识别的物理地址 (PA) 空间中穿越物理网络。该层通过在出站路径上具有封装规则和在入站路径上具有解封装规则来创建CA/PA边界。此外，状态防火墙的 ACL 层位于我们的 Ananta NAT 层之上。安全控制器已将其相对于这些边界放置在这里，知道它可以在 CA 空间中对与 VM 的 DIP 相匹配的策略进行编程。最后，用于计费的计量层位于 VM 旁边的顶部，它可以按照 VM 中的客户所看到的方式准确计量流量 - 进入 VM 的所有流量以及从 VM 发出的所有流量。

![](/img/2024-02-10-azure-vfp/figure5.jpg)

分层还为我们提供了一个实施有状态策略的良好模型。由于给定连接上的数据包在入站和出站路径上应处于相同的 IP/端口状态，因此我们可以通过假设 TCP 或 UDP 5 元组（SrcIP、DstIP、IP 协议、SrcPort）来保持层上的流状态，DstPort）将在该层的每一侧相反，并将其编码在任一方向上所有连接的哈希表中。当有状态规则匹配时，会在分层流表中创建入站流和出站流，规则方向的流执行该规则的动作，相反方向的流执行相反的动作，以保持分层。这些入站和出站流被认为是成对的 - 它们的操作只是将数据包更改为该对中相反流的状态，而不是携带自己的操作上下文。

在处理数据包时，VFP 通过搜索组来在每一层中搜索单个规则以进行匹配。层内的规则用于匹配规则。然后对该数据包执行该规则的操作 - 只有一个规则可以匹配给定层中的给定数据包（其他较低优先级的匹配规则将被忽略）。

### 5.3 Rules
- - -

规则是对 MAT 模型中匹配的数据包执行操作的实体。根据目标#3，规则允许控制器尽可能具有表现力，同时最大限度地减少数据平面中的固定策略。规则由两部分组成：通过条件列表指定的条件列表和操作，两者均在下面进行描述。

#### 5.3.1 Conditions
- - -

当 VFPAPI 客户端编写规则时，它会提供带有条件列表的描述符。条件具有类型（例如源 IP 地址）和匹配值列表（每个值可以是单例、范围或前缀）。对于匹配数据包的条件，任何匹配值都可以匹配（OR 子句）。对于要匹配的规则，规则中的所有条件都非常匹配（AND 子句）。

#### 5.3.2 Actions
- - -

规则描述符也有一个动作。该操作包含类型和特定于该类型的数据结构，以及执行规则所需的数据（例如，封装规则将源/目标 IP 地址、源/目标 MAC、封装格式和密钥作为输入数据）封装数据包）。操作接口是可扩展的 - 图 6 中列出了示例条件和操作。

规则通过简单的回调接口（Initialize、Process Packet、Deinitialize）来实现，从而使基础VFP平台易于扩展。如果规则类型支持有状态实例化，流程处理程序也会在层中创建一对流 - 流也是类型化的，并且具有与规则类似的回调接口。有状态规则包括流的生存时间，这是它创建的流在最后一个数据包匹配后将保留在流表中的时间（除非由第 6.4.2 节中描述的 TCP 状态机显式过期）。

![](/img/2024-02-10-azure-vfp/figure6.jpg)

#### 5.3.3 User Defined Actions
- - -

除了我们创建的大量操作之外与此同时，在 VFPv2 中，我们添加了用户定义的操作来进一步实现目标 #3 – 允许控制器使用标头字段操作语言创建自己的规则类型（标头换位，请参阅第 6.1.2 节）。这允许扩展基本 VFP 操作集，而无需编写代码来实现数据路径中的操作。

### 5.4 Groups
- - -

出于管理目的，层上的规则被组织成逻辑组。组是 VFP 中策略的原子单元 - 客户端可以通过事务方式更新它们。在对数据包进行分类时，VFP 会遍历一层中的组，以查找每个组中与数据包匹配的最高优先级规则。默认情况下，VFP 将选择列表中最后一组匹配的规则。规则可以标记为“终止”，这意味着如果它匹配，它将立即应用，而无需遍历其他组。组可以有像规则一样的条件 - 如果组的条件不匹配，VFP 将跳过它。

* 对于具有 Docker 式容器 [35] 的虚拟机，每个虚拟机都有自己的 IP，可以通过在每个容器上设置 IP 条件来创建和管理组。
* 对于我们的状态防火墙，基础设施 ACL 和客户 ACL 可以表示为一层中的两组。阻止规则将被标记为终止 - 如果任一组阻止它，则数据包将被丢弃。只有当两组规则都允许数据包时，数据包才会通过。

除了基于优先级的匹配之外，各个组还可以根据条件类型（例如目标 IP）进行最长前缀匹配，以支持路由方案。这是作为压缩特里树实现的。

### 5.5 Resources
- - -

MAT 是用于编程一般网络策略的良好模型，但其本身并不适合所有场景，尤其是存在异常事件的场景。 VNET 需要对出站流量进行 CA->PA 查找（使用目录系统）。对于如此大的映射表来说，单独的规则并不是最佳的。因此，我们支持通用资源的可扩展模型——在本例中是映射的哈希表。资源是端口范围的结构，端口上的任何规则都可以引用。另一个例子是范围列表，它可以实现 Ananta 中描述的形式的动态源 NAT 规则。

#### 5.5.1 Event Handling / Lookups
- - -

许多存在查找未命中的 SDN 应用程序都需要快速事件 API。我们通常在资源上下文中处理事件 - 例如如果 encap 规则查找 PA/CA 映射资源但未命中，VFPAPI 客户端可以使用异步 I/O 和事件注册有效的回调机制。我们对 Ananta NAT 端口耗尽使用相同的机制。

## 6. 报文处理与流编译器
- - -

## 7. 交换模型
- - -

## 8. 注意事项
- - -

## 9. 硬件 Offload 与性能
- - -

## 10. 经验
- - -

## 11. 结论与未来工作
- - -


## 参考
- - -
* [VFP: A Virtual Switch Platform for Host SDN in the Public Cloud](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf)

## 公众号：Flowlet
- - -

<img src="/img/qrcode_flowlet.jpg" width = 30% height = 30% alt="Flowlet" align=center/>

- - -