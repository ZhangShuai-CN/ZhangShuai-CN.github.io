---
layout:     post
title:      "VFP：公有云主机 SDN 的虚拟交换平台 [NSDI'17]"
subtitle:   "公有云主机 SDN 的虚拟交换平台"
description: "VFP: A Virtual Switch Platform for Host SDN in the Public Cloud"
excerpt: ""
date:       2024-02-10 01:01:01
author:     "张帅"
image: "/img/2024-02-10-azure-vfp/background.jpg"
showtoc: true
draft: true
tags:
    - vSwitch
categories: [ Tech ]
URL: "/2024/02/10/azure-vfp/"
---

- - -
###### 关于作者
> 
> **`张帅，网络从业人员，公众号：Flowlet`**
> 
> **`个人博客：https://flowlet.net/`**
- - -

## 序言
- - -

公有云的 Host vSwitch 由于涉及的业务场景复杂（几乎所有的业务场景都跟 vSwitch 有关，例如：VPC、Nat、EIP、LB、NFV、容器场景等等），配置变更频繁，性能要求极高。同时由于国内公有云业务极其内卷，Iaas 服务基本上就是比拼价格，事实上已经沦为卖铁。VM 超卖严重，分配给 Host 用于网络处理的 CPU 与内存资源极其有限。

Host vSwitch 的设计就如同带着镣铐跳舞，基于开源的 Open vSwitch 不能满足超大规模云网络的技术指标（单 Region 支持百万级 VPC，单 VPC 支持百万级计算节点）要求，因此国内的 Top 级互联网云厂商 Host vSwitch 都采用了自研架构。从 0 到 1 设计 vSwitch 需要熟悉所有的公有云网络业务场景，并且需要极强的技术功底与网络架构能力。

本文翻译自 NSDI'17 论文《[VFP: A Virtual Switch Platform for Host SDN in the Public Cloud](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf)》，该文章介绍了微软 Azure 在设计 Host vSwitch（VFP：Virtual Filtering Platform）方面的经验。

## 前言
- - -

由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。

## 摘要
- - -

许多现代可扩展云网络架构都依赖主机网络来实施 VM 网络策略 ———— 例如，用于虚拟网络的隧道、用于负载均衡的 NAT、带状态的 ACL、QoS 等。我们推出虚拟过滤平台 (VFP：Virtual Filtering Platform) ———— 为 Microsoft Azure（大型公共云）提供该网络策略支持的一种可编程虚拟交换机。根据我们的运营经验我们为可编程虚拟交换机定下了以下几个主要目标，包括支持多个独立的网络控制器、基于连接而不是仅仅基于数据包的网络策略、通过高效缓存和分类算法来提高性能以及将流策略高效的卸载到可编程 NIC，并演示 VFP 是如何实现这些目标的。VFP 已部署在超过 100 万台运行 IaaS 和 PaaS 工作负载的主机上超过 4 年。我们将介绍 VFP 及其 API 设计、用于流处理的流语言和编译器、性能结果以及多年来在 Azure 中部署和使用 VFP 的经验。

## 1. 介绍
- - -

Amazon Web Services（AWS）、Microsoft Azure（Azure） 和 Google Cloud Platform（GCP） 等公有云工作的兴起造就了超大规模的数据中心，根据云服务供应商的公开资料显示服务器数量已经达到数百万台。这些云服务供应商不仅要为客户提供弹性伸缩和高性能的虚拟机（VM），还必须提供丰富的网络语义，例如，为客户提供的独立网络地址空间的专用虚拟网络、可扩展的四层 LB、安全组和 ACL 、虚拟路由表、限速（bandwidth metering）、QoS 等等。

这些策略非常复杂，通常无法非常经济地在传统硬件核心路由器上大规模部署。相反，一种常见的方法是在 VM 所在主机上的软件中实施这些策略（在将 VM 连接到外部网络的虚拟交换机 (vswitch) 中实施），该方法可以随着服务器数量而扩展，并使得物理网络简单、可扩展。由于该模型将集中式控制面与主机上的数据面分开，因此它被广泛认为是软件定义网络 (SDN) 的一个示例，特别是基于主机的 SDN。

作为大型公共云服务供应商，Azure 基于主机 SDN 构建其云网络，使用主机 SDN 来实现我们所提供的几乎所有的虚拟网络功能。近年来围绕 SDN 的大部分关注焦点一直都集中在致力于构建可扩展且灵活的网络控制器，虽然这一点至关重要，但是可编程 vswitch 的设计同样重要。因为云中的工作负载对成本和性能十分敏感，因此高度可编程数据面具有高性能和低开销的要求，高性能与低开销本身就是相互冲突的两个需求。

在本文中，我们提出虚拟过滤平台（VFP：Virtual Filtering Platform）———— 我们在所有主机上运行的云级虚拟交换机。VFP 之所以如此命名，是因为它充当 VM 上每个虚拟 NIC 的过滤引擎，允许控制器对其 SDN 策略进行编程。我们的目标是展示 VFP 的设计思路以及在大规模生产环境中运行 VFP 的经验。

### 1.1 相关工作
- - -

在本文中，我们通过文献中的两个例子，来展示如何通过 VFP 支持示例中的策略。第一个示例是 VL2，它通过使用主机间的无状态隧道创建虚拟网络 (VNET)。第二个示例是 Ananta，一个可扩展的 4 层 LB（Layer-4 load
balancer），它通过在终端主机的 vswitch 中运行 LB-NAT（load balancing NAT）来进行扩展，使网络内的 LB 无状态且可扩展。

此外，我们还对 OpenFlow 和 OpenVswitch（实现 OpenFlow 协议的开源 vswitch ）进行了参考比较，这是 SDN 领域的两个开创性的项目。我们从公有云的角度指出了两者之间的核心设计差异，我们希望能在更广泛的社区分享这些经验教训。

## 2. 设计目标与基本原理
- - -

VFP 的设计根据我们运行大型公共云平台的经验随着时间的推移而不断发展。 VFP 不是我们最初的 vswitch，其最初的功能也不是主机网络中的新颖想法 – VL2 和 Ananta 已经率先使用了 vswitch。

最初，我们在 Windows Hyper-V 虚拟机管理程序之上为每个主机功能构建了网络过滤驱动程序，并将其链接在一个 vswitch 中 - 用于 ACL 的状态防火墙驱动程序、用于 VL2 VNET 的隧道驱动程序、用于 Ananta 负载平衡的 NAT 驱动程序、 QoS 驱动程序等。随着主机网络成为我们虚拟化策略的主要工具，在得出为主机网络功能构建新的固定过滤器驱动程序不可扩展的结论后，我们决定在 2011 年创建 VFP 或理想的。相反，我们创建了一个基于由 OpenFlow [5] 等项目推广的匹配动作表 (MAT) 模型的单一平台。这就是我们针对 VFP 客户端的 VFPAPI 编程模型的起源。

VFP 的核心设计目标是从构建和运行这些过滤器以及它们之上的网络控制器和代理的经验教训中汲取的。

### 2.1 最初的目标
- - -

以下是 VFP 项目的创立目标：
> 1. 提供一个编程模型，允许多个同时、独立的网络控制器对网络应用程序进行编程，从而最大限度地减少跨控制器依赖性。

OpenFlow 和类似 MAT 模型的实现通常假设一个分布式网络控制器负责对交换机进行编程（可能从其他控制器获取输入）。我们的经验是，这种模型不适合 SDN 的云开发——相反，独立团队经常为这些应用程序构建新的网络控制器和代理。与向现有控制器添加逻辑相比，该模型减少了复杂的依赖关系，可更好地扩展并且更可用。我们需要一种设计，不仅允许控制器独立创建和编程流表，而且能够在它们之间强制执行良好的分层和边界（例如，不允许规则对其他表进行任意 GOTO，如 OpenFlow 中），以便可以开发新的控制器添加功能，无需旧控制器考虑其行为。

> 2. 提供能够使用连接作为基本原语的 MAT 编程模型，而不仅仅是数据包 - 作为第一类对象的状态规则。

OpenFlow 的原始 MAT 模型历史上源自编程交换或路由 ASIC，因此假设由于可用的硬件资源，数据包分类必须是无状态的。然而，我们发现我们的控制器需要连接策略，而不仅仅是数据包 - 例如，最终用户经常发现使用有状态访问控制列表 (ACL)（例如允许出站连接，但不允许入站连接）而不是无状态 ACL 来保护其虚拟机更有用用于商业交换机。控制器还需要 NAT（例如 Ananta）和其他状态策略。有状态策略在软交换机中比在 ASIC 中更容易处理，我们相信我们的 MAT 模型应该利用这一点。

> 3. 提供一个编程模型，允许控制器定义自己的策略和操作，而不是为预定义的场景实施固定的网络策略集。

由于 OpenFlow 提供的 MAT 模型的限制（历史上，一组有限的操作、有限的规则可扩展性和无表类型），OpenFlow 交换机例如 OVS 在 MAT 模型之外添加了虚拟化功能。例如，构建虚拟网络是通过 OVSDB [8] 中的虚拟隧道端点 (VTEP) 模式 [29] 完成的，而不是指定要封装 (encap) 和解封装 (decap) 哪些数据包以及如何执行的规则。

相反，我们更喜欢将所有功能基于 MAT 模型，尝试将尽可能多的逻辑推送到控制器中，同时将核心数据平面保留在 vswitch 中。例如，可以使用匹配适当条件的可编程封装和解封规则来实现 VNET，而不是定义 VNET 的模式，并将 VNET 的定义保留在控制器中。我们发现这大大减少了每次 VNET 定义发生变化时不断扩展数据平面的需要。

P4 语言 [9] 尝试为交换机或 vswitch [38] 实现类似的目标，但非常通用，例如允许动态定义新的标头。由于我们更新 vswitch 的频率远高于向网络添加新数据包标头的频率，因此我们更喜欢预编译的快速标头解析器库的速度，以及面向有状态连接的处理而构建的语言。

### 2.2 基于生产学习的目标
- - -

根据 VFP 初始部署的经验教训，我们为 VFPv2 添加了以下目标，这是 2013-14 年的一次重大更新，主要围绕可维护性和性能：

> 4. 提供可维护性模型，允许频繁部署和更新，无需重新启动或中断虚拟机连接即可实现状态流和强大的服务监控。

随着我们的规模急剧增长（从 O(10K) 主机到 O(1M) 主机）、更多基于 VFP 构建的控制器以及更多工程师加入我们，我们发现对频繁更新（包括功能和错误修复）的需求比以往任何时候都多。在基础设施即服务 (IaaS) 模型中，我们还发现客户不能容忍单个虚拟机停机进行更新。使用我们复杂的有状态流模型来实现这一目标更具挑战性，而跨更新维护该模型并不容易。

> 5. 通过广泛的缓存，即使有大量的表和规则，也能提供非常高的数据包速率。

随着时间的推移，我们发现随着主机 SDN 模型变得越来越流行，越来越多的网络控制器被构建，很快我们就部署了大量流表（10+），每个流表都有许多规则，降低了性能，因为数据包必须遍历每个表。与此同时，主机上的虚拟机密度不断增加，推动我们从 1G 到 10G 再到 40G，甚至更快的网卡。我们需要找到一种在不影响性能的情况下扩展到更多策略的方法，并得出结论，我们需要执行流操作的编译跨表，并使用广泛的流缓存，这样现有流上的数据包将匹配预编译的操作，而无需遍历表。`针对具有大量规则和表的情况提供快速的数据包分类算法。`

虽然解决目标 #5 显着提高了现有流的性能（例如，SYN 后的所有 TCP 数据包），但我们发现一些应用程序将数千条规则推送到其流表中（例如，与客户对等的分布式路由器 BGP，使用 VFP 作为它的 FIB），这减慢了我们的流程编译器的速度。我们需要设计一个高效的数据包分类器来处理这些情况的性能。

> 6. 实施一种有效的机制，将流策略卸载到可编程 NIC，而无需假设复杂的规则处理。

当我们扩展到 40G+ NIC 时，我们希望将策略卸载到 NIC 本身以支持 SR-IOV [22, 23]，并让 NIC 在应用相关 VFP 策略时直接向 VM 指示数据包。然而，随着控制器创建更多具有更多规则的流表，我们得出的结论是，直接卸载这些表将需要服务器级 NIC 极其昂贵的硬件资源（例如大型 TCAM、串联匹配）。因此，我们不需要尝试卸载分类操作，而是想要一个能够与我们预编译的精确匹配流很好地配合的卸载模型，要求硬件仅支持访问 DRAM 中的大型缓存流表，并支持我们相关的操作语言。

### 2.3 非目标
- - -

以下是我们在其他项目中看到的目标，根据我们的经验，我们选择不追求这些目标：

> 1. 提供跨平台的可移植性。

通过内核中的高性能数据路径很难实现可移植性。 OVS 等项目通过策略将其分为内核快速路径和可移植用户空间慢速路径来实现此目的，但这样做的代价是，当数据包采用慢速路径时，速度会降低一个数量级以上 [16]。我们完全在一个主机操作系统上运行，所以这不是我们的目标。

> 2. 支持与VFP本身捆绑的网络/远程配置协议。

OpenFlow 既包含网络编程模型，也包含用于配置网络规则的有线协议。 OVS和OVSDB协议也是如此。为了启用管理策略的不同控制器模型（例如规则推送模型或 VL2 目录系统拉取模型），我们将 VFP 作为 vswitch 与实现网络配置协议的代理解耦，并专注于提供高性能主机API。

> 3. 提供检测或预防机制控制器编程冲突策略

许多文献[17-21]描述了检测或防止流表或规则匹配系统中的冲突或不正确策略的尝试。尽管我们的第一个目标是支持多个控制器并行编程 VFP 而不会互相干扰，但我们很早就得出结论，由于多种原因，显式冲突管理既不是可行的也不是必要的目标。代表虚拟机对 VFP 进行编程是一项受保护的操作，只有我们的控制器才能执行，因此我们不担心恶意控制器。此外，我们得出的结论是，不可能区分错误编程的流表（意外覆盖另一个流表的操作）和设计用于过滤另一个表的输出的流表之间的区别。相反，我们专注于帮助开发人员验证他们的策略以及与其他策略的交互的工具。

## 3. 概述
- - -

## 4. 过滤模型
- - -

## 5. 编程模型
- - -

## 6. 报文处理与流编译器
- - -

## 7. 交换模型
- - -

## 8. 注意事项
- - -

## 9. 硬件 Offload 与性能
- - -

## 10. 经验
- - -

## 11. 结论与未来工作
- - -


## 参考
- - -
* [VFP: A Virtual Switch Platform for Host SDN in the Public Cloud](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf)

## 公众号：Flowlet
- - -

<img src="/img/qrcode_flowlet.jpg" width = 30% height = 30% alt="Flowlet" align=center/>

- - -