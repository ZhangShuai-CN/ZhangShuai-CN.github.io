---
layout:     post
title:      "VFP：公有云主机 SDN 的虚拟交换平台 [NSDI'17]"
subtitle:   "公有云主机 SDN 的虚拟交换平台"
description: "VFP: A Virtual Switch Platform for Host SDN in the Public Cloud"
excerpt: ""
date:       2024-02-10 01:01:01
author:     "张帅"
image: "/img/2024-02-10-azure-vfp/background.jpg"
showtoc: true
draft: true
tags:
    - vSwitch
categories: [ Tech ]
URL: "/2024/02/10/azure-vfp/"
---

- - -
###### 关于作者
> 
> **`张帅，网络从业人员，公众号：Flowlet`**
> 
> **`个人博客：https://flowlet.net/`**
- - -

## 序言
- - -

公有云的 Host vSwitch 由于涉及的业务场景复杂（几乎所有的业务场景都跟 vSwitch 有关，例如：VPC、Nat、EIP、LB、NFV、容器场景等等），配置变更频繁，性能要求极高。同时由于国内公有云业务极其内卷，Iaas 服务基本上就是比拼价格，事实上已经沦为卖铁。VM 超卖严重，分配给 Host 用于网络处理的 CPU 与内存资源极其有限。

Host vSwitch 的设计就如同带着镣铐跳舞，基于开源的 Open vSwitch 并不能满足超大规模云网络的技术指标（单 Region 支持百万级 VPC，单 VPC 支持百万级计算节点）要求，因此国内的 Top 级互联网云厂商 Host vSwitch 都采用了自研架构。从 0 到 1 设计 vSwitch 需要熟悉所有的公有云网络业务场景，并且需要极强的技术功底与网络架构能力。

本文翻译自 NSDI'17 论文《[VFP: A Virtual Switch Platform for Host SDN in the Public Cloud](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf)》，该文章介绍了微软 Azure 云在设计 Host vSwitch（VFP：Virtual Filtering Platform）方面的经验。

## 前言
- - -

由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。

## 摘要
- - -

许多现代可扩展云网络架构都依赖主机网络来实施 VM 网络策略 ———— 例如，用于虚拟网络的隧道、用于负载均衡的 NAT、带状态的 ACL、QoS 等。我们推出虚拟过滤平台 (VFP：Virtual Filtering Platform) ———— 为 Microsoft Azure（大型公共云）提供该网络策略支持的一种可编程虚拟交换机。根据我们的运营经验我们为可编程虚拟交换机定下了以下几个主要目标，包括支持多个独立的网络控制器、基于连接而不是仅仅基于数据包的网络策略、通过高效缓存和分类算法来提高性能以及将流策略高效的卸载到可编程 NIC，并演示 VFP 是如何实现这些目标的。VFP 已部署在超过 100 万台运行 IaaS 和 PaaS 工作负载的主机上超过 4 年。我们将介绍 VFP 及其 API 设计、用于流处理的流语言和编译器、性能结果以及多年来在 Azure 中部署和使用 VFP 的经验。

## 1. 介绍
- - -

Amazon Web Services（AWS）、Microsoft Azure（Azure） 和 Google Cloud Platform（GCP） 等公有云工作的兴起造就了超大规模的数据中心，根据云服务供应商的公开资料显示服务器数量已经达到数百万台。这些云服务供应商不仅要为客户提供弹性伸缩和高性能的虚拟机（VM），还必须提供丰富的网络语义，例如，为客户提供的独立网络地址空间的专用虚拟网络、可扩展的四层 LB、安全组和 ACL 、虚拟路由表、限速（bandwidth metering）、QoS 等等。

这些策略非常复杂，通常无法非常经济地在传统硬件核心路由器上大规模部署。相反，一种常见的方法是在 VM 所在主机上的软件中实施这些策略（在将 VM 连接到外部网络的虚拟交换机 (vswitch) 中实施），该方法可以随着服务器数量而扩展，并使得物理网络简单、可扩展。由于该模型将集中式控制面与主机上的数据面分开，因此它被广泛认为是软件定义网络 (SDN) 的一个示例，特别是基于主机的 SDN。

作为大型公共云服务供应商，Azure 基于主机 SDN 构建其云网络，使用主机 SDN 来实现我们所提供的几乎所有的虚拟网络功能。近年来围绕 SDN 的大部分关注焦点一直都集中在致力于构建可扩展且灵活的网络控制器，虽然这一点至关重要，但是可编程 vswitch 的设计同样重要。因为云中的工作负载对成本和性能十分敏感，因此高度可编程数据面具有高性能和低开销的要求，高性能与低开销本身就是相互冲突的两个需求。

在本文中，我们提出虚拟过滤平台（VFP：Virtual Filtering Platform）———— 我们在所有主机上运行的云级虚拟交换机。VFP 之所以如此命名，是因为它充当 VM 上每个虚拟 NIC 的过滤引擎，允许控制器对其 SDN 策略进行编程。我们的目标是展示 VFP 的设计思路以及在大规模生产环境中运行 VFP 的经验。

### 1.1 相关工作
- - -

在本文中，我们通过文献中的两个例子，来展示如何通过 VFP 支持示例中的策略。第一个示例是 VL2，它通过使用主机间的无状态隧道创建虚拟网络 (VNET)。第二个示例是 Ananta，一个可扩展的 4 层 LB（Layer-4 load
balancer），它通过在终端主机的 vswitch 中运行 LB-NAT（load balancing NAT）来进行扩展，使网络内的 LB 无状态且可扩展。

此外，我们还对 OpenFlow 和 OpenVswitch（实现 OpenFlow 协议的开源 vswitch ）进行了参考比较，这是 SDN 领域的两个开创性的项目。我们从公有云的角度指出了两者之间的核心设计差异，我们希望能在更广泛的社区分享这些经验教训。

## 2. 设计目标与基本原理
- - -

VFP 的设计根据我们运行大型公有云平台的经验随着时间而不断的发展。VFP 并不是我们最初的 vswitch，在主机网络中实现 vswitch 也不是什么开创性的想法（早在 VL2 和 Ananta 中就已经提出了在主机网络中实现 vswitch 的想法）。

最初，我们在 Windows Hyper-V Hpervisor（虚拟机管理程序）之上为每个主机功能构建了网络过滤驱动（用于 ACL 的状态防火墙驱动、用于 VL2 VNET 的隧道驱动、用于 Ananta LB 的 NAT 驱动、 QoS 驱动等），并将其链接到 vswitch 中。随着主机网络成为我们虚拟化策略的主要工具，在得出为主机网络功能新构建固定过滤驱动程序不可扩展的结论后，我们决定在 2011 年创建 VFP。相反，我们创建了一个基于匹配动作表（MAT：Match-Action Table）模型的单一平台。这就是我们针对 VFP client 而设计的 VFP API 编程模型的由来。

VFP 的核心设计思想是从构建和运行过滤器（filter）以及在这些过滤器之上的控制器（network controllers）和代理（agent）的经验中得到的。

### 2.1 最初的目标
- - -

以下是 VFP 项目创立之初的目标：

> 1. 提供一个同时允许多个独立的网络控制器对网络应用进行编程的编程模型，最大限度地减少跨控制器的依赖性。

OpenFlow 和与此类似的 MAT 模型通常假设单一分布式网络控制器负责对交换机进行编程（也可能从其他控制器中获得输入）。我们的经验是，这种模型并不适合云中 SDN 的开发 ———— 相反，相互独立的开发团队经常为这些不同的网络应用构建不同的控制器和 agent。与通过向现有控制器添加逻辑相比，该模型减少了复杂的依赖关系，可以提供更好地可扩展性（scales better）和可维护性（more serviceable）。我们需要有一种设计，不仅允许控制器能够独立创建流表并对其进行编程，还要它们之间能够有良好的分层和边界（例如，不允许规则像 OpenFlow 中的那样随意的 GOTO 到其他表），以便可以通过开发新控制器来添加新功能，无需考虑旧控制器的行为逻辑。

> 2. 提供一个能够使用连接（带状态的规则）而不仅仅是数据包作为基本原语的 MAT 编程模型。

OpenFlow 的原始 MAT 模型历史上源自可编程交换机或路由器的 ASIC，因此假设由于硬件资源有限，数据包分类必须是无状态的。然而，我们发现我们的控制器需要基于连接，而不仅仅是基于数据包的策略 ———— 例如，最终用户经常发现通过使用有状态的访问控制列表（ACL）（例如，允许出站连接，但不允许入站连接）比在商用交换机中使用无状态的 ACL 来保护其虚拟机安全更加有用。控制器同样还需要 NAT（例如，Ananta）和其他有状态的策略。在软件 vswitch 中比在 ASIC 中更容易实现有状态的策略处理，我们的 MAT 模型应该利用这一点优势。

> 3. 提供一个允许控制器自定义自己的策略（policy）和动作（action）的编程模型，而不仅仅是提供只针对预定义场景的固定策略集。

由于 OpenFlow 提供的 MAT 模型的局限性（一组有限的 aciton 集、rule 有限的可扩展性、无表类型），OpenFlow 交换机（例如，OVS）在 MAT 模型之外添加了虚拟化功能。例如，通过 OVSDB 中的 VTEP（虚拟隧道端点：virtual tunnel endpoint）模式构建虚拟网络，而不是通过规则指定要封装（encap）和解封装（decap）哪些数据包以及如何执行此操作。

相反，我们更喜欢所有功能都基于 MAT 模型实现，并尝试将尽可能多的业务逻辑上移到控制器，vswitch 只保留核心的数据面逻辑。例如，可以通过匹配适当条件的可编程封装和解封规则来实现 VNET，而不是通过定义什么是 VNET 的模式，将 VNET 定义保留在控制器中，这大大减少了每次 VNET 定义发生变化时都需要不断对数据面进行扩展的次数。

P4 语言尝试为交换机或 vswitch 实现类似的目标，P4 语言非常的通用，例如，允许动态（on the fly）定义新的报文头。由于我们更新 vswitch 的频率远高于在网络中定义新报文头的频率，因此我们更喜欢预编译形式的快速报文头解析库，以及面向有状态连接处理而构建的编程语言。

### 2.2 在生产环境中学到的目标
- - -

根据 VFP 最初部署的经验教训，我们对 VFPv2 增加了以下目标（这是 13 到 14 年里的一次重大更新，该目标主要围绕可维护性（serviceability）和性能两方面）：

> 4. 提供允许无需重启或中断虚拟机状态流连接即可实现频繁部署和更新的可维护模型以及提供强大的服务监控能力。

随着我们的规模急剧增长（dramatically）（主机规模从 O(10K) 到 O(1M) ），随着越来越多的控制器基于 VFP 构建，以及越来越多的工程师加入，我们对频繁更新（功能和错误修复）的需求比以往任何时候都多。在基础设施即服务（IaaS：Infrastructure as a Servic）模型中，我们还发现客户不能容忍对单个 VM 进行停机更新。使用我们的有状态流模型（stateful flow model）来实现这一目标更具有挑战性，尤其是在跨版本更新（across update）时更不容易。

> 5. 通过大量的缓存（extensive cache）设计，在即使存在大量表项和规则的情况下，也能够提供非常高的包速率。

随着时间的推移，我们发现越来越多的控制器通过主机 SDN 模型进行构建，主机 SDN 变得越来越流行。很快我们就部署了大量流表（10+），每张流表都有许多规则，因为数据包必须遍历每张表，这大大降低了转发性能。与此同时，主机上的 VM 密度不断增加，促使我们网卡速率逐步从 1G 到 10G 再到 40G，甚至更高速率。我们需要找到一种在不影响性能的情况下能够容纳更多策略的方法，并得出结论，我们需要跨表执行流动作（flow action）的编辑（compilation），并使用大量的流缓存设计，以便现有流上的数据包能够与预编辑的动作（precompiled action）相匹配，而不必遍历表。`针对具有大量规则和表的情况提供快速数据包分类算法。`

虽然目标 #5 显着的提升了现有流的性能（例如，SYN 报文后的所有 TCP 报文），但我们发现一些应用将数千条规则推送到其流表中（例如，BGP peer 是客户的分布式路由器，使用 VFP 作为它的 FIB 表），这减慢了我们的流编译器的速度。我们需要设计一个高效的数据包分类器来处理该情况下遇到的性能问题。

> 6. 实施一种将流策略卸载到可编程 NIC 的有效机制，无需考虑（assuming）复杂规则的处理。

当我们使用 40G+ NIC 时，我们希望将策略卸载到 NIC 以支持 SR-IOV 场景，让 NIC 对相关 VM 的数据包直接执行相关的 VFP 策略。然而，随着控制器创建许多具有更多规则的流表，我们得出的结论是，直接卸载这些表 服务器 NIC 需要付出极其昂贵的硬件资源（例如，大 TCAM 表、串联匹配）。因此，对比将分类操作进行 offload，我们更想得到一个能够与预编辑精确匹配流很好配合的卸载模型，要求硬件只支持访问 DRAM 中的大缓存流表，并支持我们相关的动作语义。

### 2.3 非目标
- - -

以下是我们在其他项目中看到的目标，根据我们的经验，我们选择不支持（pursue）这些目标：

> 1. 跨平台可移植。

在内核的高性能数据路径中很难实现跨平台移植。 OVS 等项目通过将其分为内核态快速路径和可移植的用户态慢速路径来实现此目的，但这样做的代价是，当数据包采用慢速路径时，速率会降低一个数量级。我们只在一种主机操作系统上运行，所以跨平台可移植不是我们的目标。

> 2. VFP 自身支持远程配置协议。

OpenFlow 既包含网络编程模型，也包含通过远程配置协议。OVS 和 OVSDB 协议也是如此。为了支持管理策略不同的控制器模型（例如，规则推送（push）模型或 VL2 目录系统拉取（pull）模型），我们将 VFP 解耦为 vswitch 与实现远程配置协议的 agent，该 agent 专注于提供高性能的 host API。

> 3. 提供检测或防止控制器编程策略冲突的机制

许多文献描述了检测或防止策略在流表或规则匹配系统中的冲突方法。尽管我们的第一个目标就是支持多个控制器对 VFP 进行并行编程而不会互相干扰，但我们很早就得出结论，由于种种原因，显式的进行冲突管理既不可行也非必要。对 VFP 进行编程是一项受保护的操作，只有我们的控制器才能执行，因此我们不必担心恶意控制器。此外，我们得出的结论是，不可能区分错误编程（misprogrammed）的流表（意外的覆盖另一个流表的动作）和一个流表被设计用于过滤另一个表的输出，它们之间的区别。相反，我们专注于开发用于帮助开发人员验证其策略的工具。

## 3. 概述与比较
- - -

作为整篇论文的一个激励示例，我们考虑一个简单的场景，需要 4 个主机策略用于云中的 O(1M) 个虚拟机。每个策略均由其自己的 SDN 控制器编程，并且需要高性能和 SR-IOV 卸载支持：VL2 型 VNET、Ananta 型负载均衡器、状态防火墙以及用于计费目的的按目的地流量计量。我们首先根据现有解决方案对此进行评估，以证明需要我们所描述的不同方法。第 4-7 节详细介绍了 VFP 的核心设计。

### 3.1 现有解决方案：Open vSwitch
- - -

虽然 Linux 和 Windows 支持多个接口之间的桥接 [26-28]（可用作 vswitch），但这些桥接不应用 SDN 策略。其他公共云（例如 Google）已经描述了使用主机 SDN 策略的[25]，但详细信息并未公开。 OVS 是当今提供基于 vswitch 的 SDN 的主要解决方案，因此（截至版本 2.5）是我们的主要比较点。

我们相信 OVS 在使可编程主机网络广泛可用方面产生了巨大的积极影响。许多 OVS 设计选择都是由 OVS 特定目标驱动的，例如跨平台支持和 Linux 内核中的发布要求1 [1]。结合 OVS 对 OpenFlow 的使用，这些设计支持通过相同协议管理虚拟交换机和物理交换机的控制器进行部署，而这并不是我们基于主机的网络模型的目标。 OVS 还支持许多对物理交换机有用的协议，例如 STP、SPBM、BFD 和 我们不使用 IGMP 监听 [3]。

然而，部分由于 OpenFlow，OVS 的某些方面使其不适合我们的工作负载：
* OVS 本身并不支持真正的独立多控制器模型，而当我们的 VL2 和 Ananta 应用程序单独控制时则需要这种模型。底层 OpenFlow 表模型不适合多控制器用例 - 表规则指定对下一个表的显式 GOTO，导致控制器将其策略绑定在一起。另外，只能正向遍历表，而多控制器场景要求报文出站报文与入站报文反向遍历表，以便报文在任一方向匹配该控制器的策略时都处于一致的状态。 VFP 通过显式表分层解决了这个问题（§5.2）。

* OVS 本身并不支持其 MAT 模型中的 NAT 之类的有状态操作，而我们的 Ananta 示例需要这种操作（我们的防火墙也是有状态的）——在这两种情况下，控制器都需要将连接作为基本原语而不是数据包进行操作。然而，OpenFlow 仅提供数据包模型。 OVS 最近添加了对将数据包发送到 Linux 连接跟踪器以启用状态防火墙的支持，但它没有作为 MAT 公开，并且不容易支持 NAT，这需要显式双向状态表，以便 NAT 在流的返回路径上反转。 VFP 通过状态层解决了这个问题（第 5.2 节）。

* OVS 的 VTEP 架构需要显式隧道接口来实现 VL2 风格的 VNET，而不是允许控制器指定自己的 encap/decap 操作，而 OpenFlow2 本身并不支持这些操作。这会在数据平面中硬编码 VNET 模型，而不是允许控制器定义 VNET 如何工作（目标 3）。在此架构中添加复杂的 VNET 逻辑（例如 ECMP 路由）可能很困难，并且需要更改 vswitch，而不是更改策略。 VFP 通过将 encap/decap 建模为操作，直接在其 MAT (§5.3) 中支持所有这些。

* OVS 不支持动态查找客户地址到物理地址映射所需的 VL2 样式目录系统。 OpenFlow 的设计缺乏以这种方式支持大型 VNET 的可扩展性 - OpenFlow 异常数据包必须全部返回到中央控制器，并且在 OVS 中，所有主机上的 VTEP 都应在映射发生变化时进行更新。这对于支持最多 1000 个主机 [30] 的 NSX/vSphere 来说是可以的，但我们发现这在我们的规模上不可用。 VFP 通过结合无模式解决了这个问题具有高效异步 I/O 异常请求（§5.5.1）的 MAT 模型，代理可以将其重定向到与控制器分开的服务。

* OVS 没有通用的卸载操作语言或 API 来支持策略组合，例如 Ananta NAT 加 VL2 encap。虽然 SR-IOV 卸载已由 NIC 供应商在 OVS 构建之上针对特定工作负载（例如 VTEP 模式）[31] 实施，但执行通用卸载需要硬件支持原始策略的复杂多表查找（例如 [ 32]）我们发现在实践中成本相当高。 VFP 的标头换位语言（第 6.1.2、9.3 节）只需在硬件中进行一次表查找即可为所有策略提供 SR-IOV 支持。

因此，我们的政策需要不同的设计。

### 3.2 VFP 设计
- - -

![](/img/2024-02-10-azure-vfp/figure1.jpg)




## 4. 过滤模型
- - -

## 5. 编程模型
- - -

## 6. 报文处理与流编译器
- - -

## 7. 交换模型
- - -

## 8. 注意事项
- - -

## 9. 硬件 Offload 与性能
- - -

## 10. 经验
- - -

## 11. 结论与未来工作
- - -


## 参考
- - -
* [VFP: A Virtual Switch Platform for Host SDN in the Public Cloud](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-firestone.pdf)

## 公众号：Flowlet
- - -

<img src="/img/qrcode_flowlet.jpg" width = 30% height = 30% alt="Flowlet" align=center/>

- - -